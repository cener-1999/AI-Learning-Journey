{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zero-Shot",
   "id": "96fb1e696fa72856"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:19:26.095397Z",
     "start_time": "2024-09-19T23:19:23.502073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "model = OpenAI()\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"], template=\"Classify the sentiment of this text: {text}\"\n",
    ")\n",
    "chain = prompt | model\n",
    "chain.invoke({\"text\": \"I hated that movie, it was terrible!\"})"
   ],
   "id": "f5ae3bcf70fe0451",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNegative'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Few-Shot",
   "id": "cc7a8b7df94def2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:24:43.538374Z",
     "start_time": "2024-09-19T23:24:40.921050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "model = OpenAI()\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"{input} -> {output}\",\n",
    "    input_variables=[\"input\", \"output\"],\n",
    ")\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"I absolutely love the new update! Everything works seamlessly.\",\n",
    "        \"output\": \"Positive\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"It's okay, but I think it could use more features.\",\n",
    "        \"output\": \"Neutral\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm disappointed with the service, I expected much better performance.\",\n",
    "        \"output\": \"Negative\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"This is an excellent book with high quality explanations.\"})"
   ],
   "id": "9fc8039a246de5fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -> Positive'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To choose examples tailored to each input, FewShotPromptTemplate can accept a SemanticSim  ilarityExampleSelector, based on embeddings rather than hardcoded examples. The Semant  icSimilarityExampleSelector automatically finds the most relevant examples for each input.  For many tasks, standard few-shot prompting works well, but there are many other techniques  and extensions when dealing with more complex reasoning tasks.",
   "id": "eff2d6adff7065b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:25:01.508450Z",
     "start_time": "2024-09-19T23:24:55.179961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma,\n",
    "    k=4,\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"What's 10+10?\"})"
   ],
   "id": "fd341f31d308b499",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -> Neutral'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CoT\n",
    "\n",
    "There are two variants of CoT:\n",
    "- **zero-shot**\n",
    "- **few-shot**\n",
    "\n",
    "\n",
    "In zero-shot CoT, we just add the instruction “Let’s think step by step!” to the prompt.  \n",
    "\n",
    "When asking an LLM to reason through a problem, it is often more effective to have it explain its  reasoning before stating the final answer. This encourages the LLM to logically think through  the problem first, rather than just guessing the answer and trying to justify it afterward. Asking  an LLM to explain its thought process aligns well with its core capabilities."
   ],
   "id": "d14b5d8ed4794e0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Zero-Shot-CoT",
   "id": "84be1863d850b182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:28:24.154981Z",
     "start_time": "2024-09-19T23:28:20.993424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "cot_instruction = \"Let's think step by step!\"\n",
    "cot_instruction2 = \"Explain your reasoning step-by-step. Finally, state the answer.\"\n",
    "\n",
    "reasoning_prompt = \"{question}\\n\" + cot_instruction\n",
    "\n",
    "prompt = PromptTemplate(template=reasoning_prompt, input_variables=[\"question\"])\n",
    "\n",
    "model = OpenAI()\n",
    "chain = prompt | model\n",
    "chain.invoke(\n",
    "        {\n",
    "            \"question\": \"There were 5 apples originally. I ate 2 apples. \"\n",
    "            \"My friend gave me 3 apples. How many apples do I have now?\",\n",
    "        })"
   ],
   "id": "b13def0c367b7b2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: Start with 5 apples\\nStep 2: Ate 2 apples\\n5 - 2 = 3 apples\\nStep 3: Friend gave 3 apples\\n3 + 3 = 6 apples\\nAnswer: You now have 6 apples.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Few-Shot-CoT",
   "id": "ce6e1c498499aa1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:35:38.025761Z",
     "start_time": "2024-09-19T23:35:33.458637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = OpenAI()\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"{input} Reason: {output}\",\n",
    "    input_variables=[\"input\", \"output\"],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"I absolutely love the new update! Everything works seamlessly.\", \n",
    "        \"output\": \"Love and absolute works seamlessly are examples of positive sentiment. Therefore, the sentiment is positive\",\n",
    "    },\n",
    "    {  \n",
    "        \"input\": \"It's okay, but I think it could use more features.\",\n",
    "        \"output\": \"It's okay is not an endorsement. The customer further thinks it should be extended. Therefore, the sentiment is neutral\",\n",
    "    }, \n",
    "    {\n",
    "        \"input\": \"I'm disappointed with the service, I expected much better performance.\",\n",
    "        \"output\": \"The customer is disappointed and expected more. This is negative\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"This is an excellent book with high quality explanations.\"})"
   ],
   "id": "c9b48a5d73f9340e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Reason: The statement is a question and does not express a sentiment. Therefore, the sentiment is neutral.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Self-Consistency\n",
    "\n",
    "With self-consistency prompting, the model generates multiple candidate answers to a question.  These are then compared against each other, and the most consistent or frequent answer is selected  as the final output. A good example of self-consistency prompting with LLMs is in the context of  fact verification or information synthesis, where accuracy is paramount."
   ],
   "id": "23426d4ee4d74a4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T00:25:35.260019Z",
     "start_time": "2024-09-20T00:25:29.647984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "solutions_template = \"\"\"\n",
    "                    Generate {num_solutions} distinct answers to this question:\n",
    "                    {question}\n",
    "\n",
    "                    Solutions:\n",
    "                    \"\"\"\n",
    "\n",
    "solutions_prompt = PromptTemplate(\n",
    "    template=solutions_template,\n",
    "    input_variables=[\"question\", \"num_solutions\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "solutions_chain = solutions_prompt | llm | StrOutputParser()\n",
    "\n",
    "consistency_template = \"\"\"\n",
    "                        For each answer in {solutions}, count the number of times it occurs. Finally,\n",
    "                         choose the answer that occurs most.\n",
    "\n",
    "                        Most frequent solution:\n",
    "                        \"\"\"\n",
    "\n",
    "consistency_prompt = PromptTemplate(template=consistency_template, input_variables=[\"solutions\"])\n",
    "consistency_chain = consistency_prompt | llm | StrOutputParser()\n",
    "\n",
    "answer_chain = solutions_chain | consistency_chain | StrOutputParser()\n",
    "\n",
    "answer_chain.invoke(\n",
    "    input={\n",
    "        'question': \"Which year was the Declaration of Independence of the United States signed?\",\n",
    "        'num_solutions': \"5\",\n",
    "    }\n",
    ")"
   ],
   "id": "383484290b5a34d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer \"The Declaration of Independence of the United States was signed in 1776\" occurs 5 times, making it the most frequent solution.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ToT\n",
    "\n",
    "In Tree-of-Thought (ToT) prompting, we generate multiple problem-solving steps or approaches  for a given prompt and then use the AI model to critique them. The critique will be based on the  model’s judgment of the solution’s suitability to the problem."
   ],
   "id": "38ab2111dbda1202"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T00:33:32.836583Z",
     "start_time": "2024-09-20T00:33:18.916115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "solutions_template = \"\"\"\n",
    "Generate {num_solutions} distinct solutions for {problem}. Consider factors like {factors}.\n",
    "\n",
    "Solutions:\n",
    "\"\"\"\n",
    "solutions_prompt = PromptTemplate(\n",
    "    template=solutions_template, input_variables=[\"problem\", \"factors\", \"num_solutions\"]\n",
    ")\n",
    "\n",
    "evaluation_template = \"\"\"\n",
    "Evaluate each solution in {solutions} by analyzing pros, cons, feasibility,\n",
    " and probability of success.\n",
    "\n",
    "Evaluations:\n",
    "\"\"\"\n",
    "evaluation_prompt = PromptTemplate(template=evaluation_template, input_variables=[\"solutions\"])\n",
    "\n",
    "reasoning_template = \"\"\"\n",
    "For the most promising solutions in {evaluations}, explain scenarios, implementation strategies,\n",
    " partnerships needed, and handling potential obstacles. \n",
    "\n",
    "Enhanced Reasoning: \n",
    "\"\"\"\n",
    "reasoning_prompt = PromptTemplate(template=reasoning_template, input_variables=[\"evaluations\"])\n",
    "\n",
    "ranking_template = \"\"\"\n",
    "Based on the evaluations and reasoning, rank the solutions in {enhanced_reasoning} from\n",
    " most to least promising.\n",
    "\n",
    "Ranked Solutions:\n",
    "\"\"\"\n",
    "ranking_prompt = PromptTemplate(template=ranking_template, input_variables=[\"enhanced_reasoning\"])\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "solutions_chain = solutions_prompt| llm |output_parser\n",
    "evaluation_chain = evaluation_prompt| llm |output_parser\n",
    "reasoning_chain =  reasoning_prompt| llm |output_parser\n",
    "ranking_chain =  ranking_prompt| llm |output_parser\n",
    "\n",
    "tot_chain = solutions_chain | evaluation_chain | reasoning_chain | ranking_chain\n",
    "\n",
    "tot_chain.invoke(\n",
    "    input= {\n",
    "        'problem': \"Prompt engineering\",\n",
    "        'factors': \"Requirements for high task performance, low token use, and few calls to the LLM\",\n",
    "        'num_solutions': 3,\n",
    "        }\n",
    ")"
   ],
   "id": "f4a16884180088da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Implementing a comprehensive training program for engineers by partnering with a professional training organization\\n2. Utilizing advanced technology tools to automate tasks by partnering with a technology vendor\\n3. Developing standardized engineering guidelines and best practices by forming a cross-functional team of experienced engineers'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
