# 神经网络的学习

> **什么是神经网络的学习？**
>
> - 学习是指从训练数据中**自动**获取**最优权重参数**的过程；
> - 为了使神经网络可以学习，导入**损失函数**这一指标，学习的目的就是<u>以损失函数为基准，找出能使它的值达到最小的权重参数</u>
> - 为了找出尽可能小的损失函数的值，利用函数斜率梯度下降法



## 从数据中学习

由数据自动决定权重参数的值



#### 数据驱动



人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入



**方案一：**  **人工实现**

**方案二：用特征量和机器学习的方法**

- 用特征量和机器学习的方法从图像中提取特征量，再用机器学习技术学习这些特征量的模式
  - 特征量：可以从输入数据中准确地提取本质数据（重要数据）的转换器
    - 图像的特征两通常表示为**向量**形式；在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等
- 使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。
- 问题：<u>特征量仍然是由人设计的，对于不同问题，需要专门设计特征量；</u>

**方案三：深度学习**

没有人为介入，图像中包含的重要特征量也是由机器来学习的；

![img](https://mypicsformarkdown.oss-cn-shanghai.aliyuncs.com/imgs/202409200101390.png)

**神经网络的优点是对所有的问题都可以用同样的流程来解决。**

神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。



#### 测试数据和训练数据

一般将数据分为训练数据和测试数据两部分

- **训练数据（监督数据）**：用于学习，存在最优参数；
- 测试数据：使用测试数据评价训练得到的模型和实际能力

**模型的泛化能力**：指处理未被观察过的数据（不包含在训练数据中的数据）的能力，获得泛化能力是机器学习的最终目标

**过拟合**：只对某个数据集过度拟合的状态，但无法处理其他数据集



## 损失函数



**损失函数（loss function）**

- 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致
- 这个损失函数可以使用任意函数，但一般用<u>均方误差</u>和<u>交叉熵误差</u>等。



#### 均方误差

 **Mean squared error**

$ \displaystyle E = \frac{1}{2} \sum_{k}{(y_k-t_k)^2}$

均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方



One-hot 表示： 把正确答案标成1，其他标成0



#### 交叉熵误差

**Cross entropy error**

$\displaystyle E = - \sum_{k} t_k\log{y_k}$



#### mini-batch 学习

使用训练数据进行学习，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。

计算损失函数时必须将所有的训练数据作为对象。也就是说，如果训练数据有100个的话，我们就要把这100个损失函数的总和作为学习的指标。

> E.g.: 以交叉熵为例：
>
> $\displaystyle E = -\frac{1}{N} \sum_{n} \sum_{k}{t_{nk} \log{y_{nk}}}$



当数据量过于大时，以全部数据为对象计算loss function是不可行的。



**mini-batch学习**：从全部数据中选出一小部分，作为全部数据的近似。神经网络的学习是从训练数据中随机选出一批数据（称为mini-batch*,*小批量），然后对每个mini-batch进行学习。



**随机抽取数据：**`np.random.choice()`可以从指定的数字中随机选择想要的数字。

>  e.g.: `np.random.choice(60000, 10)` 会从0到59999之间随机选择10个数字



#### 为什么要设定损失函数？

- **在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数**
- **为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值**



假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数:

- 此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。
- 如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。
- 不过，当导数的值为0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处



**为什么不使用识别精度作为指标？**

因为如果以识别精度为指标，则参数的导数在绝大多数地方都会是0；

识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。

仅仅微调参数，是无法改善识别精度的。即便识别精度有所改善，它的值也不会像32*.*0123 *...* %这样连续变化，而是变为33 %、34 %这样的不连续的、离散的值

这一点也和为什么使用sigmoid函数作为激活函数，不使用阶跃函数做完激活函数的原理是一样的

![img](https://mypicsformarkdown.oss-cn-shanghai.aliyuncs.com/imgs/202409200145245.png)



## 数值微分



梯度法使用梯度的信息决定前进的方向

**导数**： 描述某个瞬间的变化量

$\displaystyle \frac{df(x)}{dx} = \lim_{h\to \infty} \frac{(x+h)-f(x)}{h}$



**中心差分**：函数*f*在(*x* + *h*)和(*x* *−* *h*)之间的差分。因为这种计算方法以*x*为中心，计算它左右两边的差分，所以也称为中心差分

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)-f(x-h))/(2*h)
```



**偏导数**： 求偏导需要将多个变量中的某一个变成定为目标变量，其他变量固定为某个值

$\displaystyle \frac{\partial f} {\partial x}$



**梯度**：由全部变量的偏导数汇总而成的向量称为梯度（gradient）

**梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向**

虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。<u>因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。</u>



### 梯度法

机器学习的主要任务是在学习时寻找最优参数（权重和偏置， 最优参数是指损失函数取最小值时的参数）

**用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法**



**梯度法（gradient method）**

在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法。

梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。



**梯度法的数学表达**（以两个变量为例）

$\displaystyle x_0 =x_0 -\eta \frac{\partial{f}}{\partial{x_0}} \\ \displaystyle x_1 =x_1 -\eta \frac{\partial{f}}{\partial{x_1}}$



**学习率**: `η`表示更新量，在神经网络的学习中，称为学习率（learning rate）。

- 学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。

- 学习率需要事先确定为某个值，比如0*.*01或0*.*001。一般而言，这个值过大或过小，都无法抵达一个“好的位置”。在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。



**超参数**：

像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重 和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练 数据和学习算法自动获得的，学习率这样的<u>超参数则是人工设定的</u>。 一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利 进行的设定。



**鞍点**（saddlepoint)： 

函数的极小值、最小值以及被称为鞍点（saddlepoint)的地方， 梯度为0。极小值是局部最小值，也就是限定在某个范围内的最 小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是 极小值的点。虽然梯度法是要寻找梯度为0的地方，但是那个地 方不一定就是最小值(也有可能是极小值或者鞍点)。此外，当函 数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区， 陷入被称为“学习高原”的无法前进的停滞期。



### 神经网络的梯度

神经网络的学习也要求梯度。这里的梯度是指损失函数关于权重参数的梯度；

![img](https://mypicsformarkdown.oss-cn-shanghai.aliyuncs.com/imgs/202409200200492.png)



## 学习算法的实现



### SGD

**随机梯度下降法（stochastic gradient descent）** **SGD**

- 前提

  - 神经网络存在合适的权重和偏置
  - 调整权重和偏置以便拟合训练数据的过程称之为“学习”

- 步骤一：

  mini_batch

  - 训练集中随机选出一部分数据，这部分数据称为（mini_batch）
  - 目标是减少mini_batch的损失函数的值

- 步骤二：

  计算梯度

  - 求出各个权重参数的梯度
  - 梯度表示损失函数的值减小最多的方向

- 步骤三：

  更新参数

  - 将权重参数沿梯度方向进行微小更新

- 步骤四：重复

  - 重复步骤1~3



### 基于测试数据的评价

训练数据的损失函数值减小，是神经网络的学习正常进行的一个信号，但不能证明改神经网络在其他数据集上也能有同等程度的表现；

**过拟合**

在训练数据中可以准确识别，而在其他数据中不能识别。

神经网络的学习中，必须确认是否会发生过拟合



**评价神经网络**

神经网络学习的最初目标是掌握泛化能力，所以要评价神经网络的泛化能力必须使用不包含在训练数据中的数据

定期地训练数据和测试数据记录识别精度。每经过一个epoch，记录下训练数据和测试数据的识别精度



**epoch**

epoch是一个单位。<u>－个epoch表示学习中所有训练数据均被使用过 一次时的更新次数。</u>比如，对于10000笔训练数据，用大小为100 笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所 有的训练数据就都被“看过”了此时，，100次就是一个epocho



## 小结

- 神经网络学习的目标是：以损失函数为基准，可以找出使它的值达到最小的权重参数。为了找到尽可能小的损失函数值，我们介绍了使用函数斜率的梯度法。
- 机器学习中使用的数据集分为训练数据和测试数据。
- 神经网络用训练数据进行学习，并用测试数据评价学习到的模型的 泛化能力。 
- 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数 的值减小。 
- 利用某个给定的微小值的差分求导数的过程，称为数值微分。 
- 利用数值微分，可以计算权重参数的梯度。数值微分虽然费时间，但是实现起来很简单。
